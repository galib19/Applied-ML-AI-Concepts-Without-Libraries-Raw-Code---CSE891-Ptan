{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5 (Question 5)\n",
    "\n",
    "For this question, you will write your own implementation of the logistic regression classifier and apply it to a given input dataset. Note that you must write your own functions to perform the classification and avoid using scikit-learn's implementation or any implementation you find online. You are allowed to use the following:\n",
    "\n",
    "- numpy functions for creating and manipulating ndarrays.\n",
    "- pandas library for loading and displaying the data.\n",
    "- scikit-learn library for splitting the data into training/test sets and for model evaluation. You can only use train_test_split(), confusion_matrix(), and accuracy_score() from the library for this homework problem.\n",
    "\n",
    "If you're unsure whether you can use some function, please email the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Download the *heart.csv* file from D2L to your working directory. Load and display the data. Column names are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>bp</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>blood sugar</th>\n",
       "      <th>ecg</th>\n",
       "      <th>heart rate</th>\n",
       "      <th>angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>major vessels</th>\n",
       "      <th>thal</th>\n",
       "      <th>heart attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp   bp  cholesterol  blood sugar  ecg  heart rate  angina  \\\n",
       "0     63    1   3  145          233            1    0         150       0   \n",
       "1     37    1   2  130          250            0    1         187       0   \n",
       "2     41    0   1  130          204            0    0         172       0   \n",
       "3     56    1   1  120          236            0    1         178       0   \n",
       "4     57    0   0  120          354            0    1         163       1   \n",
       "..   ...  ...  ..  ...          ...          ...  ...         ...     ...   \n",
       "298   57    0   0  140          241            0    1         123       1   \n",
       "299   45    1   3  110          264            0    1         132       0   \n",
       "300   68    1   0  144          193            1    1         141       0   \n",
       "301   57    1   0  130          131            0    1         115       1   \n",
       "302   57    0   1  130          236            0    0         174       0   \n",
       "\n",
       "     oldpeak  slope  major vessels  thal  heart attack  \n",
       "0        2.3      0              0     1             1  \n",
       "1        3.5      0              0     2             1  \n",
       "2        1.4      2              0     2             1  \n",
       "3        0.8      2              0     2             1  \n",
       "4        0.6      2              0     2             1  \n",
       "..       ...    ...            ...   ...           ...  \n",
       "298      0.2      1              0     3             0  \n",
       "299      1.2      1              0     3             0  \n",
       "300      3.4      1              2     3             0  \n",
       "301      1.2      1              1     3             0  \n",
       "302      0.0      1              1     2             0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv( 'heart.csv',header='infer' )\n",
    "data.columns = ['age','sex','cp','bp','cholesterol','blood sugar','ecg','heart rate','angina',\n",
    "               'oldpeak','slope','major vessels','thal','heart attack']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Extract the predictor matrix, X, and a vector of the target attribute y, from the pandas dataframe. Standardize the predictor attributes by subtracting each value with its corresponding column mean and dividing by its corresponding standard deviation. **Note:** Standardization is important especially when the predictors have different range of values. Since this is a classification (not regression) problem, you need to standardize the predictor attributes only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After standardization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950624</td>\n",
       "      <td>0.679881</td>\n",
       "      <td>1.969864</td>\n",
       "      <td>0.762694</td>\n",
       "      <td>-0.255910</td>\n",
       "      <td>2.390484</td>\n",
       "      <td>-1.004171</td>\n",
       "      <td>0.015417</td>\n",
       "      <td>-0.69548</td>\n",
       "      <td>1.085542</td>\n",
       "      <td>-2.270822</td>\n",
       "      <td>-0.713249</td>\n",
       "      <td>-2.145324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.912150</td>\n",
       "      <td>0.679881</td>\n",
       "      <td>1.000921</td>\n",
       "      <td>-0.092585</td>\n",
       "      <td>0.072080</td>\n",
       "      <td>-0.416945</td>\n",
       "      <td>0.897478</td>\n",
       "      <td>1.630774</td>\n",
       "      <td>-0.69548</td>\n",
       "      <td>2.119067</td>\n",
       "      <td>-2.270822</td>\n",
       "      <td>-0.713249</td>\n",
       "      <td>-0.512075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.471723</td>\n",
       "      <td>-1.465992</td>\n",
       "      <td>0.031978</td>\n",
       "      <td>-0.092585</td>\n",
       "      <td>-0.815424</td>\n",
       "      <td>-0.416945</td>\n",
       "      <td>-1.004171</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>-0.69548</td>\n",
       "      <td>0.310399</td>\n",
       "      <td>0.974740</td>\n",
       "      <td>-0.713249</td>\n",
       "      <td>-0.512075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.179877</td>\n",
       "      <td>0.679881</td>\n",
       "      <td>0.031978</td>\n",
       "      <td>-0.662770</td>\n",
       "      <td>-0.198030</td>\n",
       "      <td>-0.416945</td>\n",
       "      <td>0.897478</td>\n",
       "      <td>1.237849</td>\n",
       "      <td>-0.69548</td>\n",
       "      <td>-0.206364</td>\n",
       "      <td>0.974740</td>\n",
       "      <td>-0.713249</td>\n",
       "      <td>-0.512075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.289984</td>\n",
       "      <td>-1.465992</td>\n",
       "      <td>-0.936965</td>\n",
       "      <td>-0.662770</td>\n",
       "      <td>2.078611</td>\n",
       "      <td>-0.416945</td>\n",
       "      <td>0.897478</td>\n",
       "      <td>0.582975</td>\n",
       "      <td>1.43311</td>\n",
       "      <td>-0.378618</td>\n",
       "      <td>0.974740</td>\n",
       "      <td>-0.713249</td>\n",
       "      <td>-0.512075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.950624  0.679881  1.969864  0.762694 -0.255910  2.390484 -1.004171   \n",
       "1 -1.912150  0.679881  1.000921 -0.092585  0.072080 -0.416945  0.897478   \n",
       "2 -1.471723 -1.465992  0.031978 -0.092585 -0.815424 -0.416945 -1.004171   \n",
       "3  0.179877  0.679881  0.031978 -0.662770 -0.198030 -0.416945  0.897478   \n",
       "4  0.289984 -1.465992 -0.936965 -0.662770  2.078611 -0.416945  0.897478   \n",
       "\n",
       "          7        8         9        10        11        12  \n",
       "0  0.015417 -0.69548  1.085542 -2.270822 -0.713249 -2.145324  \n",
       "1  1.630774 -0.69548  2.119067 -2.270822 -0.713249 -0.512075  \n",
       "2  0.975900 -0.69548  0.310399  0.974740 -0.713249 -0.512075  \n",
       "3  1.237849 -0.69548 -0.206364  0.974740 -0.713249 -0.512075  \n",
       "4  0.582975  1.43311 -0.378618  0.974740 -0.713249 -0.512075  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['heart attack']\n",
    "X = data.drop(columns=['heart attack'])\n",
    "\n",
    "# Standardize the X values\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def myMean(vec):\n",
    "    col_means = np.zeros(vec.shape[1])\n",
    "    for col_number in range (vec.shape[1]):\n",
    "        col_means[col_number] = sum(vec[:,col_number])/len(vec[:,col_number])\n",
    "    \n",
    "    return col_means\n",
    "def myStdDev(vec):\n",
    "    avg = myMean(vec)\n",
    "    col_stdevs = np.zeros(vec.shape[1])\n",
    "    for col_number in range (vec.shape[1]):\n",
    "        sum_of_square = sum((vec[i,col_number] - avg[col_number])**2 for i in range(vec.shape[0]))\n",
    "        col_stdevs[col_number] = math.sqrt(sum_of_square/(len(vec[:,col_number])-1))\n",
    "    return col_stdevs\n",
    "def calc_Mean_Stdev(arr):\n",
    "    mean_values = np.zeros( arr.shape[1] )\n",
    "    stdDev_values = np.zeros( arr.shape[1] )\n",
    "\n",
    "    mean_values = myMean(arr) \n",
    "    stdDev_values = myStdDev(arr)\n",
    "    \n",
    "    return(mean_values, stdDev_values)\n",
    "\n",
    "def standardize(arr):\n",
    "    avg = myMean(arr)\n",
    "    stdev = myStdDev(arr)\n",
    "    standardized_array = arr \n",
    "    for col_number in range (arr.shape[1]):\n",
    "        for i in range(arr.shape[0]):\n",
    "            standardized_array[i,col_number] = (arr[i,col_number] - avg[col_number])/stdev[col_number]\n",
    "    return standardized_array\n",
    "\n",
    "X = standardize(X.values)\n",
    "\n",
    "\n",
    "print('After standardization:')\n",
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Partition the data into training and test sets. Reserve 5% of the data for training and the remaining 95% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 15\n",
      "Training class distribution:\n",
      "0    0.6\n",
      "1    0.4\n",
      "Name: heart attack, dtype: float64\n",
      "\n",
      "Test set size: 288\n",
      "Test class distribution:\n",
      "1    0.552083\n",
      "0    0.447917\n",
      "Name: heart attack, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.95, random_state=1)\n",
    "\n",
    "print('Training set size:', X_train.shape[0])\n",
    "print('Training class distribution:')\n",
    "print(pd.Series(Y_train).value_counts(normalize=True))\n",
    "\n",
    "print('\\nTest set size:', X_test.shape[0])\n",
    "print('Test class distribution:')\n",
    "print(pd.Series(Y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** You need to write the code for a class named myLogistic that implements the logistic regression classifier. There are 2 functions that must be implemented in this class:\n",
    "- init() function, which initializes the hyperparameters of the classifier. \n",
    "- fit() function, which trains the classifier on some input training data provided by the user using gradient descent and subgradient descent methods.\n",
    "- predict() function, which applies the classifier to some given input data.\n",
    "\n",
    "Read carefully the instruction below to implement the class. Pay careful attention to the input arguments and return values of each function. There are 3 different loss functions that must be implemented by your logistic regression classifier:\n",
    "\n",
    "loss=None:   $\\mathcal{L} = \\sum_{i=1}^N \\bigg[ y_i \\log\\bigg(1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - w_0}\\bigg )  + (1 - y_i) \\log\\bigg(1 + e^{\\mathbf{w}^T\\mathbf{x}_i + w_0}\\bigg) \\bigg]$\n",
    "\n",
    "loss='l2':   $\\mathcal{L} = \\sum_{i=1}^N \\bigg[ y_i \\log\\bigg(1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - w_0}\\bigg )  + (1 - y_i) \\log\\bigg(1 + e^{\\mathbf{w}^T\\mathbf{x}_i + w_0}\\bigg) \\bigg] + C \\|\\mathbf{w}\\|_2^2$\n",
    "\n",
    "loss='l1':   $\\mathcal{L} = \\sum_{i=1}^N \\bigg[ y_i \\log\\bigg(1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - w_0}\\bigg )  + (1 - y_i) \\log\\bigg(1 + e^{\\mathbf{w}^T\\mathbf{x}_i + w_0}\\bigg) \\bigg] + C \\|\\mathbf{w}\\|_1$\n",
    "\n",
    "where $N$ is the size of the training data, $w_0$ is the model intercept, $\\mathbf{w}$ is the vector of model coefficients, and $C$ is the regularization parameter.\n",
    "\n",
    "For loss=None and loss='l2', you need to apply the standard gradient descent algorithm to solve the optimization problem. For loss='l1', you need to apply the subgradient descent algorithm to solve the problem. During training, you need to compute the training loss for every iteration and store them in an array. The training loss is computed as follows: \n",
    "$$\\textrm{Training loss} = \\sum_{i=1}^N \\bigg[ \\hat{y}_i - y_i\\bigg]\n",
    "= \\sum_{i=1}^N \\bigg[ \\frac{1}{1 + e^{-\\mathbf{w}^T\\mathbf{x}_i - w_0}} - y_i\\bigg],$$\n",
    "where $\\hat{y}_i$ is the predicted posterior probability for data point $x_i$. The training loss will be returned by the fit() function, which you can use to plot the convergence of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogistic():\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression classifier.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, loss = None, maxiter = 100, learning_rate = 0.01, C=1.0):\n",
    "        \"\"\"\"\n",
    "            Input:  \n",
    "                input_dim: number of predictor attributes.\n",
    "                loss: loss function, whose value is either None, 'l1', or 'l2' (default = None).\n",
    "                maxiter: maximum iteration before the gradient/subgradient descent algorithm terminates (default=100).\n",
    "                learning_rate: learning rate for gradient/subgradient descent (default = 0.01).\n",
    "                C: regularization hyperparameter for l1 and l2-regularized logistic regression.\n",
    "\n",
    "            Output: None            \n",
    "        \"\"\"\n",
    "        self.loss = loss \n",
    "        self.maxiter = maxiter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.coef = np.zeros( X.shape[1] )          # initialize model coefficients to be a vector of zeros\n",
    "        self.intercept = 0                   # initialize model intercept to be 0.\n",
    "        self.C = C                         # regularization parameter\n",
    "         \n",
    "    def __sigmoid(self, z):  \n",
    "        return 1 / (1 + np.exp(-z))  \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\"\n",
    "            Input:  \n",
    "                X: N x d matrix of predictor attributes, where N is #training points and d is #predictor attributes\n",
    "                y: N x 1 vector of target attributes, where each value in the vector is either 0 or 1.\n",
    "\n",
    "            Output: \n",
    "                loss: maxiter x 1 vector containing value of the loss function in each iteration\n",
    "        \"\"\"    \n",
    "        \n",
    "        loss = np.zeros(self.maxiter)                      # initialize the loss to be a vector of zeros\n",
    "        for i in range(self.maxiter):\n",
    "\n",
    "            z = np.dot(X, self.coef) + self.intercept\n",
    "            df_dz = (-y * np.exp(-z)) / (1 + np.exp(-z)) + ((1-y)*np.exp(z))/ (1 + np.exp(z))\n",
    "            \n",
    "            if self.loss == 'l2':       # Gradient descent update for l2-regularized logistic regression \n",
    "                \n",
    "                df_dw = np.dot(X.T, df_dz) + 2*self.C*self.coef\n",
    "                df_db = np.dot(np.ones(X.shape[0]), df_dz) + 2*self.C*self.intercept\n",
    "                self.coef -= self.learning_rate * df_dw\n",
    "                self.intercept -= self.learning_rate * df_db\n",
    "                \n",
    "                loss[i] =  np.linalg.norm(self.__sigmoid(z) - y)  # loss = P(y=1|x) - actual (sum over all n data points) \n",
    "                \n",
    "            elif self.loss == 'l1':     # Subgradient descent update for l1-regularized logistic regression\n",
    "\n",
    "                df_dw = np.dot(X.T,df_dz) + self.C*np.sign(self.coef)\n",
    "                df_db = np.dot(np.ones(X.shape[0]),df_dz) + self.C*np.sign(self.intercept)\n",
    "                self.coef -= self.learning_rate * df_dw\n",
    "                self.intercept -= self.learning_rate * df_db\n",
    "                \n",
    "                loss[i] =  np.linalg.norm(self.__sigmoid(z) - y)  # loss = P(y=1|x) - actual (sum over all n data points)\n",
    "                \n",
    "            else:                       # Gradient descent update for unregularized logistic regression\n",
    "                \n",
    "                df_dw = np.dot(X.T, df_dz)\n",
    "                df_db = np.dot(np.ones(X.shape[0]), df_dz)\n",
    "                self.coef -= self.learning_rate * df_dw\n",
    "                self.intercept -= self.learning_rate * df_db\n",
    "                \n",
    "                loss[i] =  np.linalg.norm(self.__sigmoid(z) - y)\n",
    "                \n",
    "        return loss\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\"\n",
    "            Input:  \n",
    "                X: N x d matrix of predictor attributes, where N is #data points and d is #predictor attributes\n",
    "\n",
    "            Output: \n",
    "                Y_pred: N x 1 vector containing the predicted class of each data point (either 0 or 1)\n",
    "                Y_probs: N x 2 vector containing posterior probabilities of each data point in each of the 2 classes,\n",
    "                         where Y_probs[:,0] = P(y=0|x) and Y_probs[:,1] = P(y=1|x). \n",
    "        \"\"\"    \n",
    "        \n",
    "        z = np.dot(X, self.coef) + self.intercept\n",
    "        Y_probs = np.column_stack((self.__sigmoid(-z), self.__sigmoid(z)))\n",
    "        Y_pred = (self.__sigmoid(z)).round()\n",
    "        \n",
    "        return (Y_pred, Y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Apply the logistic regression model to the training data and evaluate it on the test data. Plot the value of the loss for each iteration to check convergence of the gradient/subgradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Unregularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [ 0.8403175  -2.41364541  1.30966054 -2.48465296 -2.19314018 -0.87547159\n",
      "  0.0676435   1.94629577 -1.24805069 -0.26131881  1.77993365 -0.50717612\n",
      "  0.52009699]\n",
      "Model intercept: -0.13048858710443192\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model = myLogistic(input_dim = X_train.shape[1], maxiter=1000, loss=None)\n",
    "losses = model.fit( X_train, Y_train )\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('Logistic regression')\n",
    "print('Model coefficients:', model.coef )\n",
    "print('Model intercept:', model.intercept )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on training set:\n",
      "Accuracy: 100.0 %\n",
      "[[9 0]\n",
      " [0 6]]\n",
      "\n",
      "Model performance on test set:\n",
      "Accuracy: 75.0 %\n",
      "[[ 95  34]\n",
      " [ 38 121]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model performance on training set:')\n",
    "Ypred, Y__train_probs = model.predict( X_train )\n",
    "print('Accuracy:', accuracy_score(Y_train, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_train, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred, Y_test_probs = model.predict( X_test )\n",
    "print('Accuracy:', accuracy_score(Y_test, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_test, Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Logistic regression with 'l2' loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [ 0.05856643 -0.33299485  0.32844267 -0.43395867 -0.26045173 -0.27485793\n",
      "  0.11342607  0.24609241 -0.19496956 -0.22291188  0.31714745 -0.15605594\n",
      " -0.0093471 ]\n",
      "Model intercept: -0.0957317599319496\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaNElEQVR4nO3de5hddX3v8fdn7z2XTK6QDDEJSLgIBawghKPSUlE4CniJp/UCtQekRfScY0s9TxVaDlofL0ctXmoRORQpKghSREt9pMUjBezjQQ0UIRhREAKBXCaEkBuZZGa+54+19mTPzp6ZPclO9vzWfF7PM8/sWWvttb6/NXs+89u/ddmKCMzMLH2ldhdgZmat4UA3MysIB7qZWUE40M3MCsKBbmZWEA50M7OCcKBPMpKelHRGu+sYj6TFkkJSZQ+ff6qkR/dBXSHpyFHm3S3pwhZs468kXbuHz90i6fC9rWGyk3S1pMvbXcdUs0d/jLZ/SDof+DPgZcAm4JvAX0XEQFsLa4GI+BFwdLvr2BMR8almlpN0N3BDRAyHf0TM2Fd1TSYR8f521zAVuYc+ufUAfw7MA14FnA78RbNP3tPe8742WetKRav3n38fxeFAn8Qi4isR8aOI2BERzwA3Ar8z2vKS/lrSrZJukLQJeI+k2ZK+Kmm1pGckfUJSOV++LOlzktZLekLSB2qHUeqHf/L13zDKti+QtELSZkm/kfS+mnmnSVol6RJJa4B/qE7L578rH4qofvXnvVskdUm6QtJTktbmb+Wn1az7Q3nbnpX0x83uW0klSf9L0kpJ6yR9XdLsmvnn5fOek3R57b6o3Q+SuvP9/ZykjZJ+Jmm+pE8CpwJX5m26Ml9+eEhI0rR8/6+U9IKkf69t2zj7ryTpUkmP59u+RdKBE6h/Iq+TIyXdk9e4XtK38umS9IV8/70g6SFJL8/nXS/pEzX1vFfSY5I2SLpd0sKaeSHp/ZJ+Lel5SV+WpGZ/l7aLAz0tvwc8Ms4yS4FbgTlk/wC+BgwARwKvBN4AVMeR3wucBZwAnAi8bS9qWwe8GZgFXAB8QdKJNfNfAhwIHApcVPvEiPhWRMzIhyMWAr8BbspnfwY4Kq/xSGAR8BEASWeSvWP5z2TDUhM59vCe/Ot1wOHADKAauscCVwHvBhYAs/PtNnJ+Pv8QYC7wfuDFiLgM+BHwgbxtH2jw3CuAk4BTyPbNh4GhUbZTv//+jOz39VqyffY88OUJ1D+R18nHgTuBA4CDgb/Lp7+B7DV5VL6edwHP1Rcu6fXA/wbemdezEri5brE3AycDx+fLvXGU/WBjiQh/TaIv4EngjAbTLwBWAfPGeO5fA/fW/Dwf6Aem1Uw7F/i3/PFdwPtq5p0BBFBpVEu+/hvyx4trl21Qy3eBi/PHpwE7gO6a+acBq+qeUwK+B3wl/1nAVuCImmVeAzyRP74O+HTNvKPymo4cpaa7gQvzxz8E/nvNvKOBnWTHlT4C3FQzryev/4wG++GPgR8DrxhrezXTgiw0S8CLwPFNvCYa7b8VwOk1Py+YYP0TeZ18HbgGOLiurtcDvwJeDZTq5l0PfCJ//FXgszXzZuS1Lq7ZJ79bM/8W4NJ2/y2m+OUeegIkvQ34NHBWRKzPp727ZojijprFn655fCjQAazOhwM2Av8HOCifv7Bu+drHE63xLEn35W+pNwJnk439V/VFxPZxVvNJYCZZ7xOglyyM7q+p/1/y6Y3qXzmBkhfWLb+SLAzn1683IrbRoOeZ+wbwr8DN+bDPZyV1NLH9eUA38HiT9dbvv0OB79TslxXA4ATqn8jr5MNk/1x/KumR6tBWRNxF9q7my8BaSddImtWg9hH7OiK25PXUvmtYU/N4G1no2wQ50Ce5fFjh74G3RMTD1ekRcWPkwxQRcVbNU2pvn/k0Wc9rXkTMyb9mRcRx+fzVZG+hqw6p2/xWskCteskoNXYB3yYbQpgfEXOA75OFQKO6Gq3jHLJe4dsjYmc+eT1ZL/a4mvpnx64zRVbX1fzSsbZR51myIKt97gCwlrr9ko9rz220kojYGREfi4hjyYZO3gycV509xvbXA9uBI5qst35dT5P9g59T89Ud2bGWZupv+nUSEWsi4r0RsRB4H3BV9ThARHwpIk4CjiN7h/ShBrWP2NeSpuf1PNNk261JDvRJLB97vBH4g4j46USfHxGrycY+PydpVn4g7QhJr80XuQW4WNIiSXOAS+pW8SBwjqQOSUuAt4+yqU6gC+gDBiSdRTa+2hRJryQbl31bRPTV1D9E9s/sC5IOypddJKk6vnoL2QG9YyX1AB9tdptkY/QflHSYpBnAp4BvRXZK6K3AWySdIqkT+Bgj/znV1v46Sb+dH0DcRDaUMJjPXks2Pr+bvG3XAZ+XtFDZAerX5P8cm3E18ElJh+Z19Epams9ruv68ljFfJ5LeIan6D+J5sn8Gg5JOlvSq/B3JVrJ/UIMNNvFN4AJJJ+Tt+xTwk4h4ssm2WpMc6JPb5WQHtL4/yvBKM84jC9xfkP0x3ko23gpZWN4JPAT8B1mveoBdf5SXk/UgnycLhW822kBEbCYbJrklX/YPgdsnUONSsgNu/96gnZcAjwH35Wdk/F/y89cj4g7gi2THAh7LvzfrOrLhknuBJ8jC6E/z9T6SP76ZrLe7meygb3+D9byEbJ9uIhv2uAeongn0t8Db8zM3vtTguX8BPAz8DNhAdgC42b/JvyXbx3dK2gzcR3Zq60TrrxrrdXIy8BNJW/JtXhwRT5AdAP/7fPmVZMMoV9SvOCJ+SPZa+nZezxHAOU220yZAEf6AC8vkPeurI+LQcReeQvIe/EbgZXmQJSX1+q157qFPYcrOgz5bUkXSIrIhi++0u67JQNJbJPXk471XkPWkn2xvVc1LvX7bMw70qU1kQynPkw25rCA/x9tYSnYw71myc9zPibTezqZev+0BD7mYmRWEe+hmZgXRtpvyzJs3LxYvXtyuzZuZJen+++9fHxG9jea1LdAXL17MsmXL2rV5M7MkSRr1imgPuZiZFYQD3cysIBzoZmYF4UA3MysIB7qZWUE40M3MCsKBbmZWEMkF+qNrNvO5Ox9l/Zax7gRqZjb1JBfoj63bwt/d9RjPbdnR7lLMzCaV5AK9nFc8OOSbipmZ1Uou0EvKPklryHeJNDMbIblAL5eyQHcP3cxspGQDfcCBbmY2QrKB7iEXM7OR0gt0ecjFzKyR5AK9VO2hO9DNzEZILtCHD4p6yMXMbITkAr3kIRczs4aSC3QfFDUzayy9QB/uobe5EDOzSSa5QC/50n8zs4aSC/RKnugecjEzGym5QK/enMtXipqZjZRcoA/fnMuBbmY2QnKB7ptzmZk1llygD5+H7jF0M7MRkgv0si/9NzNrKNlAdw/dzGyk5ALdB0XNzBpLLtB9UNTMrLH0An34oGibCzEzm2TSC/RytYfum7mYmdVKL9B9cy4zs4aSC/Tqzbl8Lxczs5GSC3R/pqiZWWPpBbrPcjEzayi5QJeE5CEXM7N64wa6pOskrZO0fJT5syX9s6SfS3pE0gWtL3OksuQeuplZnWZ66NcDZ44x/38Av4iI44HTgM9J6tz70kZXKsmX/puZ1Rk30CPiXmDDWIsAMyUJmJEvO9Ca8horS77038ysTivG0K8EjgGeBR4GLo6IhmeJS7pI0jJJy/r6+vZ4g5WSfB66mVmdVgT6G4EHgYXACcCVkmY1WjAiromIJRGxpLe3d483WCrJV4qamdVpRaBfANwWmceAJ4DfasF6R1X2GLqZ2W5aEehPAacDSJoPHA38pgXrHVVJHnIxM6tXGW8BSTeRnb0yT9Iq4KNAB0BEXA18HLhe0sOAgEsiYv0+qxgol3w/dDOzeuMGekScO878Z4E3tKyiJpTlIRczs3rJXSkK2UFR99DNzEZKMtB9UNTMbHdpBrov/Tcz202SgV4qyTfnMjOrk2SgV0piwB8qamY2QpKBXpJ76GZm9ZIM9HLJY+hmZvWSDPTs9rntrsLMbHJJMtDL8pWiZmb10gx0D7mYme0myUAv+dJ/M7PdJBnoZV/6b2a2m2QDfcCBbmY2QpKBXvEYupnZbtIM9HLJPXQzszppBnpJDPgji8zMRkgz0MslD7mYmdVJM9BLYueQe+hmZrWSDfRBX/tvZjZCmoFeFjs95GJmNkKagV4q+aComVmdJAPdFxaZme0uyUDvKPsTi8zM6iUZ6OWST1s0M6uXZKB3lH3aoplZvSQDvVwSEf6QCzOzWkkGekc5K9u9dDOzXZIM9HJJAB5HNzOrkWSgV/JA3+kzXczMhiUZ6NUhF/fQzcx2STLQq0MuvlrUzGyXJAO9o5wHunvoZmbDkgz0cikr21eLmpntkmSg7+qhe8jFzKwqyUAfHkP3kIuZ2bAkA73iIRczs90kGugecjEzq5dmoPssFzOz3Ywb6JKuk7RO0vIxljlN0oOSHpF0T2tL3J2HXMzMdtdMD/164MzRZkqaA1wFvDUijgPe0ZrSRlfxWS5mZrsZN9Aj4l5gwxiL/CFwW0Q8lS+/rkW1jWp4DN09dDOzYa0YQz8KOEDS3ZLul3TeaAtKukjSMknL+vr69niDFd/LxcxsN60I9ApwEvAm4I3A5ZKOarRgRFwTEUsiYklvb++eb3D4bosecjEzq6q0YB2rgPURsRXYKule4HjgVy1Yd0PVMXT30M3MdmlFD/2fgFMlVST1AK8CVrRgvaMa7qE70M3Mho3bQ5d0E3AaME/SKuCjQAdARFwdESsk/QvwEDAEXBsRo57i2ArV0xYHfZaLmdmwcQM9Is5tYpm/Af6mJRU1oexPLDIz202SV4p2VvIPifZBUTOzYUkGevUj6HYOONDNzKqSDPRdPXQPuZiZVSUZ6NUPuNjhIRczs2FJBnpnPuSyw0MuZmbDkgx0SXSU5R66mVmNJAMdsgOjPihqZrZLsoHeWSn5tEUzsxrJBnpHueQhFzOzGskGeme5xI4Bn7ZoZlaVbqBX3EM3M6uVbKB3lOWDomZmNZINdPfQzcxGSjbQO8o+y8XMrFaygZ4dFHWgm5lVpRvoHnIxMxsh3UD3kIuZ2QjJBnqHh1zMzEZIN9ArJd8P3cysRrKB7oOiZmYjpRvoFd8+18ysVrqB7oOiZmYjJBvoPihqZjZSuoHu+6GbmY2QbKBnQy7B0JDPdDEzg4QDvbujDEC/h13MzICkAz0rffvOwTZXYmY2OSQc6FkPffuAA93MDBIO9K5KtYfuIRczM0g40Id76B5yMTMDkg70rHQfFDUzy6Qb6BX30M3MaiUb6F0ecjEzGyHZQN912qKHXMzMIOFA76pULyxyD93MDBIOdF9YZGY2UsKB7kv/zcxqJR/o7qGbmWXGDXRJ10laJ2n5OMudLGlQ0ttbV97oun2lqJnZCM300K8HzhxrAUll4DPAv7agpqZUyiXKJbmHbmaWGzfQI+JeYMM4i/0p8G1gXSuKalZ3peQeuplZbq/H0CUtAv4LcHUTy14kaZmkZX19fXu7abo7yr7boplZrhUHRb8IXBIR4yZrRFwTEUsiYklvb+9eb3haZ5ntOxzoZmYAlRasYwlwsySAecDZkgYi4rstWPeYZnRV2NI/sK83Y2aWhL0O9Ig4rPpY0vXA9/ZHmAP0dJbZ5h66mRnQRKBLugk4DZgnaRXwUaADICLGHTffl6Z3Vdi83T10MzNoItAj4txmVxYR79mraiZoemeFNS9s35+bNDObtJK9UhSyHrqHXMzMMokHetkHRc3McokHeoVtOxzoZmaQeqB3ltk5GL4nupkZqQd6V3ZMd1u/A93MLO1A78wC3ePoZmapB3q1h+4zXczM0g70nq7sQy7cQzczSzzQZwz30B3oZmZJB3pPZ9ZD3+oeuplZ2oFe7aFv9VkuZmZpB3pPfpbLVg+5mJmlHejuoZuZ7ZJ0oHd3lCgJtvTvbHcpZmZtl3SgS2L2tA5eeNGBbmaWdKADzOnpZOM2B7qZWfKB7h66mVkm+UA/oKfDPXQzMwoQ6HN6Otn44o52l2Fm1nbJB/rsaR1s3OoeuplZ8oF+QE8nm/sH2Dk41O5SzMzaKvlAn9PTAcAmHxg1symuMIG+0YFuZlNcAQK9E4CN23xg1MymtvQDfVreQ/epi2Y2xaUf6D0OdDMzKECgHzA9G3J53kMuZjbFJR/oM7sqdFVKrNvc3+5SzMzaKvlAl8T8Wd2s3bS93aWYmbVV8oEOcNDMLtZtcg/dzKa2QgT6/FndrN3sHrqZTW2FCPRe99DNzIoR6PNndbOlf4Ct/f6waDObugoS6F0APtPFzKa0QgT6QTO7AXymi5lNaYUI9GoPfc0LDnQzm7oKEegHH9ADwNMbtrW5EjOz9hk30CVdJ2mdpOWjzH+3pIfyrx9LOr71ZY5tWmeZg2Z2sdKBbmZTWDM99OuBM8eY/wTw2oh4BfBx4JoW1DVhh87t4annHOhmNnWNG+gRcS+wYYz5P46I5/Mf7wMOblFtE/LSA6ezcsPWdmzazGxSaPUY+p8Ad7R4nU05dG4Pazf1s33nYDs2b2bWdi0LdEmvIwv0S8ZY5iJJyyQt6+vra9WmgSzQAZ7yOLqZTVEtCXRJrwCuBZZGxHOjLRcR10TEkohY0tvb24pND1s8dzoAj6/b0tL1mpmlYq8DXdJLgduA/xoRv9r7kvbMUfNnIsEv12xuVwlmZm1VGW8BSTcBpwHzJK0CPgp0AETE1cBHgLnAVZIABiJiyb4qeDTTOsscNnc6K1Zv2t+bNjObFMYN9Ig4d5z5FwIXtqyivXDMglk8/MwL7S7DzKwtCnGlaNUxC2by1IZtbN7uD4w2s6mnUIF+3MLZACx/xsMuZjb1FCrQT3zpAUjwsydHvQ7KzKywChXos3s6OHr+TAe6mU1JhQp0gJMXH8gDK59nYHCo3aWYme1XhQv0U46Yy9Ydg9y/8vnxFzYzK5DCBfqpR/XSWS7xg1+sbXcpZmb7VeECfUZXhVOOnMsPVqwlItpdjpnZflO4QAc4++ULWPncNh54ysMuZjZ1FDLQ3/SKBUzvLHPTT59udylmZvtNIQN9eleFt56wiH/++bOs2+QPjjazqaGQgQ7w/tcezsBQcNXdj7e7FDOz/aKwgX7o3Om846SDueG+lTzyrG/YZWbFV9hAB7j0rN9iTk8nH/rHh/zRdGZWeIUO9Dk9nXz693+bFWs28cFvPcjgkE9jNLPiKnSgA5xx7HwuO/sY7li+hvd943629A+0uyQzs32i8IEOcOGph/Oxtx7HXb9cyxs+fw93PLyaIffWzaxgxv3EoqI4/5TFvHzRLC77znL+240PcPi86fzBSQdzxjHzOWr+DPKPzzMzS5badXn8kiVLYtmyZft9uzsHh/j+w6v52o+f5IGnNgIws7vCsQtmcdT8mSyY083C2dPondnFjK4KM7srzOiuML2zQke5REdZDn8zaxtJ94/2uc1Tpode1VEusfSERSw9YRFrN23nnkf7eOiZjSx/ZhO3//xZXnhx/I+vq5RER7lEpZx/LwkJRPU7w6Ffzf5G8wVQt7xZavzKnbh3nXwIF556eMvXO+UCvdb8Wd288+RDeOfJhwxP29o/wJpN21m/uZ8t/QNs6R9g0/YBtvYPMDA4xI7BYGBwiIGhYMfAEANDQwwMBhEQVL8z/DMAw9OiZt6un/FwviUq/OLdI/NmdO2T9U7pQG9keleFI3pncETvjHaXYmY2IVPiLBczs6nAgW5mVhAOdDOzgnCgm5kVhAPdzKwgHOhmZgXhQDczKwgHuplZQbTtXi6S+oCVe/j0ecD6FpaTArd5anCbp4a9afOhEdHbaEbbAn1vSFo22s1pisptnhrc5qlhX7XZQy5mZgXhQDczK4hUA/2adhfQBm7z1OA2Tw37pM1JjqGbmdnuUu2hm5lZHQe6mVlBJBfoks6U9KikxyRd2u56WkXSIZL+TdIKSY9IujiffqCkH0j6df79gJrn/GW+Hx6V9Mb2Vb/nJJUl/Yek7+U/F729cyTdKumX+e/6NVOgzR/MX9PLJd0kqbtobZZ0naR1kpbXTJtwGyWdJOnhfN6XNNHPpoyIZL6AMvA4cDjQCfwcOLbddbWobQuAE/PHM4FfAccCnwUuzadfCnwmf3xs3v4u4LB8v5Tb3Y49aPf/BL4JfC//uejt/RpwYf64E5hT5DYDi4AngGn5z7cA7ylam4HfA04EltdMm3AbgZ8CryH7qNY7gLMmUkdqPfT/BDwWEb+JiB3AzcDSNtfUEhGxOiIeyB9vBlaQ/TEsJQsB8u9vyx8vBW6OiP6IeAJ4jGz/JEPSwcCbgGtrJhe5vbPI/vC/ChAROyJiIwVuc64CTJNUAXqAZylYmyPiXmBD3eQJtVHSAmBWRPy/yNL96zXPaUpqgb4IeLrm51X5tEKRtBh4JfATYH5ErIYs9IGD8sWKsC++CHwYGKqZVuT2Hg70Af+QDzNdK2k6BW5zRDwDXAE8BawGXoiIOylwm2tMtI2L8sf105uWWqA3Gk8q1HmXkmYA3wb+PCI2jbVog2nJ7AtJbwbWRcT9zT6lwbRk2purkL0t/0pEvBLYSvZWfDTJtzkfN15KNrSwEJgu6Y/GekqDaUm1uQmjtXGv255aoK8CDqn5+WCyt2+FIKmDLMxvjIjb8slr87di5N/X5dNT3xe/A7xV0pNkQ2evl3QDxW0vZG1YFRE/yX++lSzgi9zmM4AnIqIvInYCtwGnUOw2V020javyx/XTm5ZaoP8MeJmkwyR1AucAt7e5ppbIj2Z/FVgREZ+vmXU7cH7++Hzgn2qmnyOpS9JhwMvIDqgkISL+MiIOjojFZL/HuyLijyhoewEiYg3wtKSj80mnA7+gwG0mG2p5taSe/DV+OtnxoSK3uWpCbcyHZTZLenW+r86reU5z2n10eA+OJp9NdgbI48Bl7a6nhe36XbK3Vw8BD+ZfZwNzgR8Cv86/H1jznMvy/fAoEzwaPpm+gNPYdZZLodsLnAAsy3/P3wUOmAJt/hjwS2A58A2yszsK1WbgJrJjBDvJetp/sidtBJbk++lx4Eryq/mb/fKl/2ZmBZHakIuZmY3CgW5mVhAOdDOzgnCgm5kVhAPdzKwgHOhmZgXhQDczK4j/D3Brl11KLBWcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = myLogistic(input_dim = X_train.shape[1], maxiter=1000, loss= 'l2' , C=2.0)\n",
    "losses = model.fit( X_train, Y_train )\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('l2-regularized logistic regression')\n",
    "print('Model coefficients:', model.coef )\n",
    "print('Model intercept:', model.intercept )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on training set:\n",
      "Accuracy: 93.33333333333333 %\n",
      "[[8 1]\n",
      " [0 6]]\n",
      "\n",
      "Model performance on test set:\n",
      "Accuracy: 75.34722222222221 %\n",
      "[[ 99  30]\n",
      " [ 41 118]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model performance on training set:')\n",
    "Ypred, Y__train_probs = model.predict( X_train )\n",
    "print('Accuracy:', accuracy_score(Y_train, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_train, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred, Y_test_probs = model.predict( X_test )\n",
    "print('Accuracy:', accuracy_score(Y_test, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_test, Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Logistic regression with l1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [ 0.00319359 -0.48877095  0.87903452 -0.9462309  -0.10824017 -0.19988558\n",
      "  0.00245712  0.33123617 -0.10079775 -0.00110695  0.23022253 -0.00963793\n",
      "  0.00932927]\n",
      "Model intercept: -0.007890370480880626\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = myLogistic(input_dim = X_train.shape[1] , maxiter=1000, loss= 'l1' , C=1.0)\n",
    "losses = model.fit( X_train, Y_train )\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('l1-regularized logistic regression')\n",
    "print('Model coefficients:', model.coef )\n",
    "print('Model intercept:', model.intercept )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on training set:\n",
      "Accuracy: 100.0 %\n",
      "[[9 0]\n",
      " [0 6]]\n",
      "\n",
      "Model performance on test set:\n",
      "Accuracy: 76.73611111111111 %\n",
      "[[ 98  31]\n",
      " [ 36 123]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model performance on training set:')\n",
    "Ypred, Y__train_probs = model.predict( X_train )\n",
    "print('Accuracy:', accuracy_score(Y_train, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_train, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred, Y_test_probs = model.predict( X_test )\n",
    "print('Accuracy:', accuracy_score(Y_test, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_test, Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Results with scikit-learn implementation (for comparison purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unregularized logistic regression\n",
      "\n",
      "Coefficient: [ 3.24044679 -7.83524208  3.40315773 -7.28615549 -7.52056892 -1.12917442\n",
      " -0.09494085  6.63099993 -4.41664855  0.63404781  5.95158719 -1.40182532\n",
      "  1.83774241]\n",
      "Intercept: 0.27562116566345657\n",
      "\n",
      "Model performance on training set:\n",
      "Accuracy: 100.0 %\n",
      "[[9 0]\n",
      " [0 6]]\n",
      "\n",
      "Model performance on test set:\n",
      "Accuracy: 73.61111111111111 %\n",
      "[[ 89  40]\n",
      " [ 36 123]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('Unregularized logistic regression\\n')\n",
    "clf = LogisticRegression(random_state=1,penalty='none',C=1.0).fit(X_train, Y_train)\n",
    "print('Coefficient:', clf.coef_[0])\n",
    "print('Intercept:', clf.intercept_[0])\n",
    "\n",
    "print('\\nModel performance on training set:')\n",
    "Ypred = clf.predict(X_train)\n",
    "print('Accuracy:', accuracy_score(Y_train, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_train, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(Y_test, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_test, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2-regularized logistic regression\n",
      "\n",
      "Coefficient: [ 0.28465694 -1.02062345  0.64754156 -1.08595545 -0.86539121 -0.44996606\n",
      "  0.1332488   0.76244128 -0.5097591  -0.31550464  0.77458497 -0.23969835\n",
      "  0.13545265]\n",
      "Intercept: -0.09462587201184484\n",
      "\n",
      "Model performance on training set:\n",
      "Accuracy: 100.0 %\n",
      "[[9 0]\n",
      " [0 6]]\n",
      "\n",
      "Model performance on test set:\n",
      "Accuracy: 75.34722222222221 %\n",
      "[[ 96  33]\n",
      " [ 38 121]]\n"
     ]
    }
   ],
   "source": [
    "print('l2-regularized logistic regression\\n')\n",
    "clf = LogisticRegression(random_state=1,penalty='l2',C=2.0,solver='liblinear').fit(X_train, Y_train)\n",
    "print('Coefficient:', clf.coef_[0])\n",
    "print('Intercept:', clf.intercept_[0])\n",
    "\n",
    "print('\\nModel performance on training set:')\n",
    "Ypred = clf.predict(X_train)\n",
    "print('Accuracy:', accuracy_score(Y_train, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_train, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(Y_test, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_test, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1-regularized logistic regression\n",
      "\n",
      "Coefficient: [ 0.         -0.48961607  0.89692346 -0.95641289 -0.10693631 -0.1957853\n",
      "  0.          0.33394573 -0.09829515  0.          0.22116773  0.\n",
      "  0.        ]\n",
      "Intercept: 0.0\n",
      "\n",
      "Model performance on training set:\n",
      "Accuracy: 100.0 %\n",
      "[[9 0]\n",
      " [0 6]]\n",
      "\n",
      "Model performance on test set:\n",
      "Accuracy: 76.73611111111111 %\n",
      "[[ 98  31]\n",
      " [ 36 123]]\n"
     ]
    }
   ],
   "source": [
    "print('l1-regularized logistic regression\\n')\n",
    "clf = LogisticRegression(random_state=1,penalty='l1',C=1.0,solver='liblinear').fit(X_train, Y_train)\n",
    "print('Coefficient:', clf.coef_[0])\n",
    "print('Intercept:', clf.intercept_[0])\n",
    "\n",
    "print('\\nModel performance on training set:')\n",
    "Ypred = clf.predict(X_train)\n",
    "print('Accuracy:', accuracy_score(Y_train, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_train, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(Y_test, Ypred)*100, '%')\n",
    "print(confusion_matrix(Y_test, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
